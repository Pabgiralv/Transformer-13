{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:42:52.005860Z","iopub.execute_input":"2022-06-21T20:42:52.006303Z","iopub.status.idle":"2022-06-21T20:42:53.790128Z","shell.execute_reply.started":"2022-06-21T20:42:52.006195Z","shell.execute_reply":"2022-06-21T20:42:53.789113Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torch\nimport torchvision.transforms as transforms\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:42:53.792182Z","iopub.execute_input":"2022-06-21T20:42:53.792806Z","iopub.status.idle":"2022-06-21T20:42:54.006542Z","shell.execute_reply.started":"2022-06-21T20:42:53.792765Z","shell.execute_reply":"2022-06-21T20:42:54.005640Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_dataset_path = '../input/pajaros/birds/birds'\n\n\ntrain_transforms = transforms.Compose ([\n    transforms.Resize((64,64)),\n    transforms.RandomVerticalFlip(),\n    transforms.CenterCrop(64),\n    transforms.ToTensor()\n])","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:42:54.010008Z","iopub.execute_input":"2022-06-21T20:42:54.010338Z","iopub.status.idle":"2022-06-21T20:42:54.017451Z","shell.execute_reply.started":"2022-06-21T20:42:54.010310Z","shell.execute_reply":"2022-06-21T20:42:54.016416Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"orig_set = torchvision.datasets.ImageFolder(root = train_dataset_path, transform = train_transforms)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:42:54.019247Z","iopub.execute_input":"2022-06-21T20:42:54.019626Z","iopub.status.idle":"2022-06-21T20:43:00.276291Z","shell.execute_reply.started":"2022-06-21T20:42:54.019576Z","shell.execute_reply":"2022-06-21T20:43:00.275444Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"n = len(orig_set)  # total number of examples\nn_test = int(0.2 * n)  # take ~10% for test\ntest_set = torch.utils.data.Subset(orig_set, range(n_test))  # take first 20%\ntrain_set = torch.utils.data.Subset(orig_set, range(n_test, n))  # take the rest\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size = 32, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size = 32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:43:00.279632Z","iopub.execute_input":"2022-06-21T20:43:00.280264Z","iopub.status.idle":"2022-06-21T20:43:00.286176Z","shell.execute_reply.started":"2022-06-21T20:43:00.280211Z","shell.execute_reply":"2022-06-21T20:43:00.285438Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        self.patch_size = patch_size\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.proj(x)  # (B, E, P, P)\n        x = x.flatten(2)  # (B, E, N)\n        x = x.transpose(1, 2)  # (B, N, E)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:43:00.287472Z","iopub.execute_input":"2022-06-21T20:43:00.287995Z","iopub.status.idle":"2022-06-21T20:43:00.297268Z","shell.execute_reply.started":"2022-06-21T20:43:00.287940Z","shell.execute_reply":"2022-06-21T20:43:00.296576Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle=True)\nbatch = next(iter(train_loader))\nimages, labels = batch\npe = PatchEmbedding(64, 16, 3, 600)\nout = pe(images)\nout.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:43:00.298123Z","iopub.execute_input":"2022-06-21T20:43:00.301574Z","iopub.status.idle":"2022-06-21T20:43:00.457035Z","shell.execute_reply.started":"2022-06-21T20:43:00.301545Z","shell.execute_reply":"2022-06-21T20:43:00.456207Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n\n    def __init__(self, n_embd, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n \n        self.key = nn.Linear(n_embd, n_embd*n_heads)\n        self.query = nn.Linear(n_embd, n_embd*n_heads)\n        self.value = nn.Linear(n_embd, n_embd*n_heads)\n\n        self.proj = nn.Linear(n_embd*n_heads, n_embd)\n\n    def forward(self, x):\n        B, L, F = x.size()\n\n  \n        k = self.key(x).view(B, L, F, self.n_heads).transpose(1, 3)\n        q = self.query(x).view(B, L, F, self.n_heads).transpose(1, 3) \n        v = self.value(x).view(B, L, F, self.n_heads).transpose(1, 3) \n\n        \n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = torch.nn.functional.softmax(att, dim=-1)\n        y = att @ v \n        y = y.transpose(1, 2).contiguous().view(B, L, F*self.n_heads) \n\n        return self.proj(y)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:43:00.458427Z","iopub.execute_input":"2022-06-21T20:43:00.458777Z","iopub.status.idle":"2022-06-21T20:43:00.469208Z","shell.execute_reply.started":"2022-06-21T20:43:00.458740Z","shell.execute_reply":"2022-06-21T20:43:00.468460Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, n_embd, n_heads):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n        self.attn = MultiHeadAttention(n_embd, n_heads)\n        self.mlp = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n        )\n\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:43:00.470597Z","iopub.execute_input":"2022-06-21T20:43:00.471707Z","iopub.status.idle":"2022-06-21T20:43:00.481355Z","shell.execute_reply.started":"2022-06-21T20:43:00.471663Z","shell.execute_reply":"2022-06-21T20:43:00.480489Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class ViT(nn.Module):\n\n    def __init__(self, img_size=64, patch_size=16, in_chans=3, embed_dim=600, n_heads=10, n_layers=6, n_classes=400):\n        super().__init__()\n\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, 1+ self.patch_embed.n_patches, embed_dim))\n        \n        self.tranformer = torch.nn.Sequential(*[TransformerBlock(embed_dim, n_heads) for _ in range(n_layers)])\n        \n        self.ln = nn.LayerNorm(embed_dim)\n        self.fc = torch.nn.Linear(embed_dim, n_classes)\n\n    def forward(self, x):\n        e = self.patch_embed(x)\n        B, L, E = e.size()\n        \n        cls_token = self.cls_token.expand(B, -1, -1)  \n        e = torch.cat((cls_token, e), dim=1)  \n        e = e + self.pos_embed \n        \n        z = self.tranformer(e)\n        \n        cls_token_final = z[:, 0]  \n        y = self.fc(cls_token_final)\n\n        return y\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:43:00.484958Z","iopub.execute_input":"2022-06-21T20:43:00.486587Z","iopub.status.idle":"2022-06-21T20:43:00.497860Z","shell.execute_reply.started":"2022-06-21T20:43:00.486545Z","shell.execute_reply":"2022-06-21T20:43:00.496975Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"vit = ViT()\nout = vit(images)\nout.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:43:00.499112Z","iopub.execute_input":"2022-06-21T20:43:00.499898Z","iopub.status.idle":"2022-06-21T20:43:04.872521Z","shell.execute_reply.started":"2022-06-21T20:43:00.499858Z","shell.execute_reply":"2022-06-21T20:43:04.871658Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:43:04.876527Z","iopub.execute_input":"2022-06-21T20:43:04.879156Z","iopub.status.idle":"2022-06-21T20:43:11.191790Z","shell.execute_reply.started":"2022-06-21T20:43:04.879111Z","shell.execute_reply":"2022-06-21T20:43:11.190800Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Model(pl.LightningModule):\n\n    def __init__(self):\n        super().__init__()\n        self.vit = ViT()\n\n    def forward(self, x):\n        return self.vit(x)\n\n    def predict(self, x):\n        with torch.no_grad():\n            y_hat = self(x)    \n            return torch.argmax(y_hat, axis=1)\n        \n    def compute_loss_and_acc(self, batch):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = (torch.argmax(y_hat, axis=1) == y).sum().item() / y.shape[0]\n        return loss, acc\n    \n    def training_step(self, batch, batch_idx):\n        loss, acc = self.compute_loss_and_acc(batch)\n        self.log('loss', loss)\n        self.log('acc', acc, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        loss, acc = self.compute_loss_and_acc(batch)\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.0003)\n        return optimizer","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:43:11.193452Z","iopub.execute_input":"2022-06-21T20:43:11.194201Z","iopub.status.idle":"2022-06-21T20:43:11.207494Z","shell.execute_reply.started":"2022-06-21T20:43:11.194154Z","shell.execute_reply":"2022-06-21T20:43:11.206678Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = Model()\nout = model(images)\nout.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:43:11.210794Z","iopub.execute_input":"2022-06-21T20:43:11.211178Z","iopub.status.idle":"2022-06-21T20:43:15.697995Z","shell.execute_reply.started":"2022-06-21T20:43:11.211136Z","shell.execute_reply":"2022-06-21T20:43:15.697193Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = Model()\ntrainer = pl.Trainer(max_epochs=20, gpus=1, logger=None)\ntrainer.fit(model, train_loader)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:43:15.699157Z","iopub.execute_input":"2022-06-21T20:43:15.700052Z","iopub.status.idle":"2022-06-21T21:47:08.670526Z","shell.execute_reply.started":"2022-06-21T20:43:15.700012Z","shell.execute_reply":"2022-06-21T21:47:08.669700Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"trainer.validate(model, dataloaders=test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T21:47:08.672025Z","iopub.execute_input":"2022-06-21T21:47:08.672394Z","iopub.status.idle":"2022-06-21T21:48:41.304925Z","shell.execute_reply.started":"2022-06-21T21:47:08.672356Z","shell.execute_reply":"2022-06-21T21:48:41.304083Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"trainer.validate(model, dataloaders=train_loader)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T21:50:13.479333Z","iopub.execute_input":"2022-06-21T21:50:13.479947Z","iopub.status.idle":"2022-06-21T21:53:42.251460Z","shell.execute_reply.started":"2022-06-21T21:50:13.479909Z","shell.execute_reply":"2022-06-21T21:53:42.250521Z"},"trusted":true},"execution_count":17,"outputs":[]}]}