{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torchvision\nimport torch\nimport torchvision.transforms as transforms\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F\nimport torch.utils.data as data\nfrom torchvision import datasets\nimport pytorch_lightning as pl\nimport glob\nfrom pandas.core.common import flatten\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-27T17:44:18.506359Z","iopub.execute_input":"2022-06-27T17:44:18.506743Z","iopub.status.idle":"2022-06-27T17:44:18.515022Z","shell.execute_reply.started":"2022-06-27T17:44:18.506706Z","shell.execute_reply":"2022-06-27T17:44:18.513315Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_dataset_path = '../input/iais22-birds/birds/birds'\n\n#NORMALIZACIÓN: image = (image - mean) /std\nmean = [0.4704, 0.4669, 0.3898]\nstd = [0.2035, 0.2001, 0.2047]\nnumber_of_images = 58388\n\ntrain_transforms = transforms.Compose ([\n    transforms.Resize((64,64)),\n    transforms.RandomHorizontalFlip(), #Para que no los datos no estén sesgados, le aplicamos modificaciones\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))\n])","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:44:18.587327Z","iopub.execute_input":"2022-06-27T17:44:18.588496Z","iopub.status.idle":"2022-06-27T17:44:18.597215Z","shell.execute_reply.started":"2022-06-27T17:44:18.588454Z","shell.execute_reply":"2022-06-27T17:44:18.595970Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"orig_set = torchvision.datasets.ImageFolder(root = train_dataset_path, transform = train_transforms) ","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:44:18.712880Z","iopub.execute_input":"2022-06-27T17:44:18.714226Z","iopub.status.idle":"2022-06-27T17:44:24.417002Z","shell.execute_reply.started":"2022-06-27T17:44:18.714116Z","shell.execute_reply":"2022-06-27T17:44:24.415809Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"n = len(orig_set)  # total number of examples\nn_test = int(0.2 * n)  # take ~10% for test\ntrain_set_size = int(0.8*n)\nvalid_set_size = n - train_set_size\ntrain_set, valid_set = data.random_split(orig_set, [train_set_size, valid_set_size])\ntrain_loader = torch.utils.data.DataLoader(orig_set, batch_size = 32, shuffle=True,pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(valid_set, batch_size = 32, shuffle=False,pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:44:24.419825Z","iopub.execute_input":"2022-06-27T17:44:24.420389Z","iopub.status.idle":"2022-06-27T17:44:24.435068Z","shell.execute_reply.started":"2022-06-27T17:44:24.420331Z","shell.execute_reply":"2022-06-27T17:44:24.433821Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:44:24.436778Z","iopub.execute_input":"2022-06-27T17:44:24.437928Z","iopub.status.idle":"2022-06-27T17:44:24.445086Z","shell.execute_reply.started":"2022-06-27T17:44:24.437887Z","shell.execute_reply":"2022-06-27T17:44:24.443592Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        self.patch_size = patch_size\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) #Aplicamos Capa Convolucional para realizar transformación lineal a todos los parches.\n\n    def forward(self, x):\n        x = self.proj(x)  # (B, E, P, P)\n        x = x.flatten(2)  # (B, E, N)\n        x = x.transpose(1, 2)  # (B, N, E)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:44:24.449350Z","iopub.execute_input":"2022-06-27T17:44:24.450054Z","iopub.status.idle":"2022-06-27T17:44:24.459767Z","shell.execute_reply.started":"2022-06-27T17:44:24.450013Z","shell.execute_reply":"2022-06-27T17:44:24.458211Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n\n    def __init__(self, n_embd, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n         #proyeccion de la key,Query y Value\n        self.key = nn.Linear(n_embd, n_embd * n_heads)#se multiplca por el num de cabezas de la red si usamos num de cabezas pequeño como en este caso\n        self.query = nn.Linear(n_embd, n_embd * n_heads)\n        self.value = nn.Linear(n_embd, n_embd * n_heads)\n\n        self.proj = nn.Linear(n_embd * n_heads, n_embd)\n\n    def forward(self, x):\n        B, L, F = x.size()\n      \n\n  \n        k = self.key(x).view(B, L, F, self.n_heads).transpose(1, 3)\n        q = self.query(x).view(B, L, F, self.n_heads).transpose(1, 3) \n        v = self.value(x).view(B, L, F, self.n_heads).transpose(1, 3) \n\n        #calculamos matriz de atención multiplicando la q por la K.T despues se realiza el escalado y por último la funcion softmax para que todas las filas sumen uno\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = torch.nn.functional.softmax(att, dim=-1)\n        y = att @ v \n        y = y.transpose(1, 2).contiguous().view(B, L, F*self.n_heads) \n\n        return self.proj(y)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:44:24.461909Z","iopub.execute_input":"2022-06-27T17:44:24.462727Z","iopub.status.idle":"2022-06-27T17:44:24.477420Z","shell.execute_reply.started":"2022-06-27T17:44:24.462671Z","shell.execute_reply":"2022-06-27T17:44:24.476185Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, n_embd, n_heads):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(n_embd)  #aplica normalización\n        self.ln2 = nn.LayerNorm(n_embd)\n        self.attn = MultiHeadAttention(n_embd, n_heads)\n        self.mlp = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),                  #aplica transformaciones lineales\n            nn.ReLU(),                                      #fn de activación\n            nn.Linear(4 * n_embd, n_embd),\n        )\n\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))#Primero normalizamos y dsp realizamos la atención\n        x = x + self.mlp(self.ln2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:44:24.479128Z","iopub.execute_input":"2022-06-27T17:44:24.480015Z","iopub.status.idle":"2022-06-27T17:44:24.491979Z","shell.execute_reply.started":"2022-06-27T17:44:24.479954Z","shell.execute_reply":"2022-06-27T17:44:24.490639Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class ViT(nn.Module):\n\n    def __init__(self, img_size=64, patch_size=8, in_chans=3, embed_dim=512, n_heads=4, n_layers=6, n_classes=400):\n        super().__init__()\n\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))  #usamos como primera entrada para clasificador del transformer class_token\n        self.pos_embed = nn.Parameter(torch.zeros(1, 1+ self.patch_embed.n_patches, embed_dim))#posicional embending, posicion del parche\n        \n        self.tranformer = torch.nn.Sequential(*[TransformerBlock(embed_dim, n_heads) for _ in range(n_layers)]) #el bloque de la red neuronal se repite la cantida d de capas que tenga nuestro modelo\n        \n        self.ln = nn.LayerNorm(embed_dim)\n        self.fc = torch.nn.Linear(embed_dim, n_classes)\n\n    def forward(self, x):\n        e = self.patch_embed(x)\n        B, L, E = e.size()\n        \n        cls_token = self.cls_token.expand(B, -1, -1)  \n        e = torch.cat((cls_token, e), dim=1)  #cat sirve para concatenar tensores\n        e = e + self.pos_embed \n        \n        z = self.tranformer(e)\n        \n        cls_token_final = z[:, 0]  #cogemos solo el primer vector que es el que predice\n        y = self.fc(cls_token_final)\n\n        return y\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:44:24.493895Z","iopub.execute_input":"2022-06-27T17:44:24.494719Z","iopub.status.idle":"2022-06-27T17:44:24.509602Z","shell.execute_reply.started":"2022-06-27T17:44:24.494666Z","shell.execute_reply":"2022-06-27T17:44:24.508289Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"vit = ViT()\n#out = vit(images)\n#out.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:44:24.512751Z","iopub.execute_input":"2022-06-27T17:44:24.514089Z","iopub.status.idle":"2022-06-27T17:44:24.906602Z","shell.execute_reply.started":"2022-06-27T17:44:24.514047Z","shell.execute_reply":"2022-06-27T17:44:24.905324Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class Model(pl.LightningModule):\n\n    def __init__(self):\n        super().__init__()\n        self.vit = ViT()\n\n    def forward(self, x):\n        return self.vit(x)\n\n    def predict(self, x):\n        with torch.no_grad():\n            y_hat = self(x)    \n            return torch.argmax(y_hat, axis=1)\n        \n    def compute_loss_and_acc(self, batch):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = (torch.argmax(y_hat, axis=1) == y).sum().item() / y.shape[0]\n        return loss, acc\n    \n    def training_step(self, batch, batch_idx):\n        loss, acc = self.compute_loss_and_acc(batch)\n        self.log('loss', loss)\n        self.log('acc', acc, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        loss, acc = self.compute_loss_and_acc(batch)\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n        return optimizer","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:44:24.908745Z","iopub.execute_input":"2022-06-27T17:44:24.909141Z","iopub.status.idle":"2022-06-27T17:44:24.921867Z","shell.execute_reply.started":"2022-06-27T17:44:24.909098Z","shell.execute_reply":"2022-06-27T17:44:24.920314Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model = Model()\n#out = model(images)\n#out.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:44:24.928469Z","iopub.execute_input":"2022-06-27T17:44:24.929433Z","iopub.status.idle":"2022-06-27T17:44:25.247642Z","shell.execute_reply.started":"2022-06-27T17:44:24.929289Z","shell.execute_reply":"2022-06-27T17:44:25.246466Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"\ntrainer = pl.Trainer(max_epochs=2, gpus=1, logger=None)\ntrainer.fit(model, train_loader,test_loader)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T18:56:09.639648Z","iopub.execute_input":"2022-06-27T18:56:09.640654Z","iopub.status.idle":"2022-06-27T19:13:03.086473Z","shell.execute_reply.started":"2022-06-27T18:56:09.640586Z","shell.execute_reply":"2022-06-27T19:13:03.085210Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"trainer.validate(model, dataloaders=train_loader)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T18:53:58.282927Z","iopub.execute_input":"2022-06-27T18:53:58.283816Z","iopub.status.idle":"2022-06-27T18:54:05.566445Z","shell.execute_reply.started":"2022-06-27T18:53:58.283756Z","shell.execute_reply":"2022-06-27T18:54:05.565273Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"trainer.validate(model, dataloaders=test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T18:54:05.568500Z","iopub.execute_input":"2022-06-27T18:54:05.568992Z","iopub.status.idle":"2022-06-27T18:54:06.813351Z","shell.execute_reply.started":"2022-06-27T18:54:05.568950Z","shell.execute_reply":"2022-06-27T18:54:06.811992Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/iais22-birds/submission_test/submission_test'\ntest_images_paths = []\nfor data_path in glob.glob(test_path + '/*'):\n    test_images_paths.append(glob.glob(data_path))\n    \ntest_images_paths = list(flatten(test_images_paths))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T18:54:06.819075Z","iopub.execute_input":"2022-06-27T18:54:06.822547Z","iopub.status.idle":"2022-06-27T18:54:07.841368Z","shell.execute_reply.started":"2022-06-27T18:54:06.822503Z","shell.execute_reply":"2022-06-27T18:54:07.840190Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def to_device(data, device):\n    \"Move data to the device\"\n    if isinstance(data,(list,tuple)):\n        return [to_device(x,device) for x in data]\n    return data.to(device,non_blocking = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T18:54:07.844032Z","iopub.execute_input":"2022-06-27T18:54:07.844900Z","iopub.status.idle":"2022-06-27T18:54:07.853323Z","shell.execute_reply.started":"2022-06-27T18:54:07.844857Z","shell.execute_reply":"2022-06-27T18:54:07.851657Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"model =model.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T18:54:07.855167Z","iopub.execute_input":"2022-06-27T18:54:07.855907Z","iopub.status.idle":"2022-06-27T18:54:07.868851Z","shell.execute_reply.started":"2022-06-27T18:54:07.855849Z","shell.execute_reply":"2022-06-27T18:54:07.867610Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"Categories = []\nId = []\n\nfor i in range (0, len(test_images_paths)):\n    img_name = test_images_paths[i]\n    img = Image.open(img_name)\n    img = train_transforms(img)\n    img = to_device(img.unsqueeze(0), device)\n    x = img_name.replace('../input/iais22-birds/submission_test/submission_test/', '')\n    x = x.split('.')\n    Id.append(x[0])\n    class_predict = model.predict(img)\n    class_predict=orig_set.classes[class_predict[0].item()]\n    Categories.append(class_predict)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T18:54:07.871762Z","iopub.execute_input":"2022-06-27T18:54:07.872565Z","iopub.status.idle":"2022-06-27T18:54:28.275108Z","shell.execute_reply.started":"2022-06-27T18:54:07.872523Z","shell.execute_reply":"2022-06-27T18:54:28.271900Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nimg_name = []\nseries_name = []\n\n\ndict = {\"Id\": Id, \"Category\": Categories}\n\ndf = pd.DataFrame(dict)\n\ndf.to_csv('submit2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T18:54:28.283905Z","iopub.execute_input":"2022-06-27T18:54:28.287335Z","iopub.status.idle":"2022-06-27T18:54:28.311690Z","shell.execute_reply.started":"2022-06-27T18:54:28.287287Z","shell.execute_reply":"2022-06-27T18:54:28.308231Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model_path = 'transformer.pth'\ntorch.save(model.state_dict(),model_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T18:54:28.312836Z","iopub.execute_input":"2022-06-27T18:54:28.313463Z","iopub.status.idle":"2022-06-27T18:54:28.903446Z","shell.execute_reply.started":"2022-06-27T18:54:28.313391Z","shell.execute_reply":"2022-06-27T18:54:28.901901Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"model2 = Model()\nmodel2.load_state_dict(torch.load(model_path))\n\nmodel2.eval()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:34:20.052159Z","iopub.execute_input":"2022-06-27T13:34:20.052447Z","iopub.status.idle":"2022-06-27T13:34:20.529303Z","shell.execute_reply.started":"2022-06-27T13:34:20.052419Z","shell.execute_reply":"2022-06-27T13:34:20.528330Z"}}}]}